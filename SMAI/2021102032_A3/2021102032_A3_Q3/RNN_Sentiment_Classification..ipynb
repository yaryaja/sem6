{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xagfwKcbRL1",
        "outputId": "fdc9a8b2-cf5f-4402-89cd-d8cb20c35b12"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXUixPHTa_jV",
        "outputId": "f91f0e9c-c5f3-46bf-e340-56e4b8c457f0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "import numpy as np\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from collections import Counter\n",
        "import matplotlib.pyplot as plt\n",
        "# import wandb\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "IPC9ip-Xa_jW"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def map_word_to_index(reviews,size):\n",
        "    word_counter=Counter([word for review in reviews for word in review])\n",
        "    common_words=word_counter.most_common(size)\n",
        "    word_to_index={word:ind+1 for ind,word in enumerate(common_words)}\n",
        "    word_to_index['<UNK>']=0\n",
        "    return word_to_index\n",
        "\n",
        "\n",
        "def preprocessed_dataset(data,labels,vocab_size,max_length):\n",
        "    stemmer=PorterStemmer()\n",
        "    tokenized_reviews=[]\n",
        "    for review in data:\n",
        "        tokens=word_tokenize(review)\n",
        "        stemmed_tokens=[stemmer.stem(token)for token in tokens]\n",
        "        tokenized_reviews.append(stemmed_tokens)\n",
        "    word_to_index=map_word_to_index(tokenized_reviews,vocab_size)\n",
        "\n",
        "    ##converting the reviews to word indices and padding them\n",
        "    indexed_reviews=[]\n",
        "    for review in tokenized_reviews:\n",
        "        indexed_review=[word_to_index[word]if word in word_to_index else 0 for word in review]\n",
        "        indexed_reviews.append(indexed_review)\n",
        "    ## creating an empty 2 d array\n",
        "    Padded_reviews=np.zeros((len(indexed_reviews),max_length))\n",
        "    for ind,review in enumerate(indexed_reviews):\n",
        "        Padded_reviews[ind, :min(len(review),max_length)]=review[:max_length]\n",
        "\n",
        "    return Padded_reviews,word_to_index\n",
        "\n",
        "\n",
        "\n",
        "## defining the architecture of the model\n",
        "\n",
        "\n",
        "\n",
        "# Load the IMDb dataset from CSV\n",
        "imdb_data = pd.read_csv('/content/drive/My Drive/SMAI_A3/IMDB Dataset.csv')\n",
        "reviews=imdb_data[\"review\"]\n",
        "labels=imdb_data[\"sentiment\"]\n",
        "\n",
        "\n",
        "\n",
        "## parameters\n",
        "max_length=300\n",
        "hidden_size=128\n",
        "output_size=1\n",
        "batch_size=64\n",
        "num_epochs=10\n",
        "learning_rate=0.0001\n",
        "word_embedding_dimension=1024\n",
        "# ### data preprocessing\n",
        "vocab_size=15000\n",
        "preprocessed_data,word_to_index=preprocessed_dataset(reviews,labels,vocab_size,max_length)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ph59syHLa_jX",
        "outputId": "c2b9ee68-5b5f-4fc0-b29b-908c0bbbda15"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "15001\n"
          ]
        }
      ],
      "source": [
        "encoded_labels = [1 if label =='positive' else 0 for label in labels]\n",
        "encoded_labels = np.array(encoded_labels)\n",
        "X_train,X_temp,Y_train,Y_temp=train_test_split(preprocessed_data,encoded_labels,test_size=0.2,random_state=0)\n",
        "\n",
        "X_val, X_test, Y_val, Y_test = train_test_split(X_temp, Y_temp, test_size=0.5, random_state=42)\n",
        "print(len(word_to_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "FqlSCd6ya_jX"
      },
      "outputs": [],
      "source": [
        "class custom_dataset(Dataset):\n",
        "    def __init__(self,reviews,labels):\n",
        "        self.labels=labels\n",
        "        self.reviews=reviews\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    def __getitem__(self,idx):\n",
        "        return torch.LongTensor(self.reviews[idx]),torch.FloatTensor([self.labels[idx]])\n",
        "\n",
        "\n",
        "## creating the dataloader using the preprocessed data\n",
        "train_dataset=custom_dataset(X_train,Y_train)\n",
        "train_loader=DataLoader(train_dataset,batch_size=64,shuffle=False)\n",
        "val_dataset=custom_dataset(X_val,Y_val)\n",
        "val_loader=DataLoader(val_dataset,batch_size=64,shuffle=False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "57qB9kWJa_jX"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "class RNN(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size, word_embedding_dimension):\n",
        "        super(RNN, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, hidden_size)\n",
        "        self.rnn = nn.RNN(hidden_size, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # print(\"entered\")\n",
        "        embeddings = self.embedding(X)\n",
        "\n",
        "        rnn_out, _ = self.rnn(embeddings)\n",
        "        output = rnn_out[:, -1, :]\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "class LSTM(nn.Module):\n",
        "    def __init__(self, vocab_size, hidden_size, output_size, word_embedding_dimension):\n",
        "        super(LSTM, self).__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, word_embedding_dimension)\n",
        "        self.lstm = nn.LSTM(word_embedding_dimension, hidden_size)\n",
        "        self.fc = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    def forward(self, X):\n",
        "        # print(\"entered\")\n",
        "        embeddings = self.embedding(X)\n",
        "\n",
        "        lstm_out, _ = self.lstm(embeddings)\n",
        "        output = lstm_out[:, -1, :]\n",
        "        output = self.fc(output)\n",
        "        return output\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "gFr0pjePa_jY"
      },
      "outputs": [],
      "source": [
        "def training_model(model,train_loader,val_loader,criterion,optimizer,epochs):\n",
        "    train_losses=[]\n",
        "    val_losses=[]\n",
        "    train_accuracies=[]\n",
        "    val_accuracies=[]\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs,labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            # print(outputs)\n",
        "            # print(labels)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                outputs = model(inputs)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct_val / total_val\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "                f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, '\n",
        "                f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "        # wandb.log({\"train_loss\": epoch_train_loss, \"train_accuracy\": epoch_train_accuracy,\n",
        "                    # \"val_loss\": epoch_val_loss, \"val_accuracy\": epoch_val_accuracy})\n",
        "    return train_losses, val_losses, train_accuracies, val_accuracies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oBEStVbga_jZ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T9aRmnAvlCCQ",
        "outputId": "e190c357-641e-4cf3-ffa3-e2b181ab8ee8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training started\n",
            "\n",
            "Epoch 1/5, Train Loss: 2.1673, Train Acc: 0.5291, Val Loss: 2.1808, Val Acc: 0.5094\n",
            "\n",
            "\n",
            "Epoch 2/5, Train Loss: 2.1669, Train Acc: 0.5291, Val Loss: 2.1805, Val Acc: 0.5094\n",
            "\n",
            "\n",
            "Epoch 3/5, Train Loss: 2.1803, Train Acc: 0.5292, Val Loss: 2.2932, Val Acc: 0.5067\n",
            "\n",
            "\n",
            "Epoch 4/5, Train Loss: 2.1802, Train Acc: 0.5290, Val Loss: 2.3932, Val Acc: 0.5094\n",
            "\n",
            "\n",
            "Epoch 5/5, Train Loss: 2.1022, Train Acc: 0.5191, Val Loss: 2.6932, Val Acc: 0.5094\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs=5\n",
        "saved=False\n",
        "\n",
        "if saved==True:\n",
        "    the_model= RNN(vocab_size+1, hidden_size, output_size,word_embedding_dimension)\n",
        "\n",
        "    the_model.load_state_dict(torch.load(\"give your file path\",map_location=torch.device(\"cpu\")))\n",
        "\n",
        "    the_model.eval()\n",
        "\n",
        "    device=torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    input_review=input(\"Enter the Review:\")\n",
        "    seq=input_review.split()\n",
        "    # with torch.no_grad():\n",
        "\n",
        "\n",
        "else:\n",
        "    model= RNN(vocab_size+1, hidden_size, output_size,word_embedding_dimension)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    loss_function=nn.BCEWithLogitsLoss()\n",
        "    optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "    print(\"training started\")\n",
        "    train_losses=[]\n",
        "    val_losses=[]\n",
        "    train_accuracies=[]\n",
        "    val_accuracies=[]\n",
        "    for epoch in range(num_epochs):\n",
        "        # print(\"epoch\",epoch,\"is running\\n\")\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs,labels in train_loader:\n",
        "            inputs=inputs.to(device)\n",
        "            labels=labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs=inputs.to(device)\n",
        "                labels=labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_function(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct_val / total_val\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "                f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, '\n",
        "                f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Hyperparamerer tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# embedd_set = [4,8,16,32,64]\n",
        "# batch_set = [8,16,32,64,128]\n",
        "# test_accuracies = []\n",
        "# for embedd in embedd_set:\n",
        "#     model =  model= RNN(vocab_size+1, hidden_size, output_size,word_embedding_dimension)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "observation:\n",
        "with increasing batch size accuracy increases and then again decrasing\n",
        "\n",
        "again with increasing embeding dimenstion observation is same.\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xz7XfNSCxz8l",
        "outputId": "08bfa68e-fe96-4b54-8980-40f06634aa8c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "training started\n",
            "\n",
            "Epoch 1/3, Train Loss: 0.0298, Train Acc: 0.6090, Val Loss: 0.0036, Val Acc: 0.5437\n",
            "\n",
            "\n",
            "Epoch 2/3, Train Loss: 0.0025, Train Acc: 0.6271, Val Loss: 0.0021, Val Acc: 0.5879\n",
            "\n",
            "\n",
            "Epoch 3/3, Train Loss: 0.0018, Train Acc: 0.6398, Val Loss: 0.0015, Val Acc: 0.5941\n",
            "\n"
          ]
        }
      ],
      "source": [
        "num_epochs=3\n",
        "saved=False\n",
        "\n",
        "if saved==True:\n",
        "    the_model= LSTM(vocab_size+1, hidden_size, output_size,word_embedding_dimension)\n",
        "\n",
        "    the_model.load_state_dict(torch.load(\"give your file path\",map_location=torch.device(\"cpu\")))\n",
        "\n",
        "    the_model.eval()\n",
        "\n",
        "    device=torch.device(\"cpu\")\n",
        "\n",
        "\n",
        "    input_review=input(\"Enter the Review:\")\n",
        "    seq=input_review.split()\n",
        "    # with torch.no_grad():\n",
        "\n",
        "\n",
        "else:\n",
        "    model= LSTM(vocab_size+1, hidden_size, output_size,word_embedding_dimension)\n",
        "\n",
        "    use_cuda = torch.cuda.is_available()\n",
        "    device = torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "\n",
        "    if use_cuda:\n",
        "        model.cuda()\n",
        "\n",
        "    loss_function=nn.BCELoss()\n",
        "    optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "    print(\"training started\")\n",
        "    train_losses=[]\n",
        "    val_losses=[]\n",
        "    train_accuracies=[]\n",
        "    val_accuracies=[]\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_train_loss = 0.0\n",
        "        correct_train = 0\n",
        "        total_train = 0\n",
        "\n",
        "        for inputs,labels in train_loader:\n",
        "            inputs=inputs.to(device)\n",
        "            labels=labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = loss_function(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "\n",
        "            running_train_loss += loss.item() * inputs.size(0)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total_train += labels.size(0)\n",
        "            correct_train += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_train_loss = running_train_loss / len(train_loader.dataset)\n",
        "        epoch_train_accuracy = correct_train / total_train\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accuracies.append(epoch_train_accuracy)\n",
        "\n",
        "        model.eval()\n",
        "        running_val_loss = 0.0\n",
        "        correct_val = 0\n",
        "        total_val = 0\n",
        "\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for inputs, labels in val_loader:\n",
        "                inputs=inputs.to(device)\n",
        "                labels=labels.to(device)\n",
        "                outputs = model(inputs)\n",
        "                loss = loss_function(outputs, labels)\n",
        "\n",
        "                running_val_loss += loss.item() * inputs.size(0)\n",
        "                _, predicted = torch.max(outputs, 1)\n",
        "                total_val += labels.size(0)\n",
        "                correct_val += (predicted == labels.squeeze(1)).sum().item()\n",
        "\n",
        "        epoch_val_loss = running_val_loss / len(val_loader.dataset)\n",
        "        epoch_val_accuracy = correct_val / total_val\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accuracies.append(epoch_val_accuracy)\n",
        "\n",
        "        print(f'Epoch {epoch+1}/{num_epochs}, '\n",
        "                f'Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_accuracy:.4f}, '\n",
        "                f'Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_accuracy:.4f}')\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Classification accracy on test set:\n",
        "\n",
        "\tRNN 0.5094\n",
        "\tLSTM 0.5941"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pUhko8C3zTyM"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zbdgF60bxAfl"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IDyWszqixCXz",
        "outputId": "958a8057-cc45-49ed-ec3b-82beca6e5aec"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vZzAPxDrt6fO",
        "outputId": "e4e246fc-b9fd-4a96-d565-f05aec84e46d"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ApsSjJ1byVGE"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
