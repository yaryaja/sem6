{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchvision import datasets,transforms\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "from skimage.metrics import structural_similarity as ssim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data=pd.DataFrame(pd.read_csv(\"./mnist_test.csv\"))\n",
    "train_data=pd.DataFrame(pd.read_csv(\"./mnist_train.csv\"))\n",
    "# check=torch.tensor(np.array(train_data.iloc[:1,1:]))\n",
    "# print(((train_data.iloc[:1,1:]).shape))\n",
    "# plt.imshow(check.reshape((28,28)))\n",
    "X=train_data.iloc[:,1:].values/255\n",
    "\n",
    "X=(X-0.5)/0.5 ## Normalization\n",
    "\n",
    "Y=train_data.iloc[:,0]\n",
    "\n",
    "# print(Y.shape)\n",
    "# print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x,val_x,train_y,val_y=train_test_split(X,Y,test_size=0.2)\n",
    "X_train_tensor = torch.tensor(train_x, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(train_y, dtype=torch.long)\n",
    "X_val_tensor = torch.tensor(np.array(val_x), dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(np.array(val_y), dtype=torch.long)\n",
    "\n",
    "\n",
    "train_dataset=TensorDataset(X_train_tensor,y_train_tensor)\n",
    "val_dataset=TensorDataset(X_val_tensor,y_val_tensor)\n",
    "batch_size=64\n",
    "train_loader=DataLoader(train_dataset,batch_size=64,shuffle=True)\n",
    "test_loader=DataLoader(val_dataset,64,shuffle=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_noise(images,sigma):\n",
    "    noise=torch.randn_like(images)*sigma\n",
    "    noisy_images=images+noise\n",
    "    return torch.clamp(noisy_images,0,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_average_ssim(model, test_loader, sigma):\n",
    "    model.eval()\n",
    "    total_ssim = 0.0\n",
    "    for images, _ in test_loader:\n",
    "        noisy_images = add_noise(images, sigma)\n",
    "        reconstructed_images = model(noisy_images)\n",
    "        for i in range(len(images)):\n",
    "            total_ssim += ssim(images[i].squeeze().numpy(), reconstructed_images[i].squeeze().detach().numpy(),data_range=2)\n",
    "    return total_ssim / len(test_loader.dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "784\n"
     ]
    }
   ],
   "source": [
    "print(len(X_train_tensor[10]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f70b546cc50>"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plt.im(X_train_tensor[0])\n",
    "plt.imshow(X_train_tensor[0].reshape((28,28)))\n",
    "plt.imshow(X_train_tensor[0].reshape((28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "class AE1(torch.nn.Module):\n",
    "\tdef __init__(self,bottleneck_layer=18):\n",
    "\t\tsuper().__init__()\n",
    "\t\t\n",
    "\t\t# Building an linear encoder with Linear\n",
    "\t\t# layer followed by Relu activation function\n",
    "\t\t# 784 ==> 9\n",
    "\t\tself.bottleneck_layer=bottleneck_layer\n",
    "\t\tself.encoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(28 * 28, 256),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(256, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, bottleneck_layer),\n",
    "\t\t\t\n",
    "\t\t)\n",
    "\t\t\n",
    "\t\n",
    "\t\tself.decoder = torch.nn.Sequential(\n",
    "\t\t\ttorch.nn.Linear(bottleneck_layer, 64),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(64, 128),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(128,256),\n",
    "\t\t\ttorch.nn.ReLU(),\n",
    "\t\t\ttorch.nn.Linear(256, 28 * 28),\n",
    "\t\t\ttorch.nn.Sigmoid()\n",
    "\t\t)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\tencoded = self.encoder(x)\n",
    "\t\tdecoded = self.decoder(encoded)\n",
    "\t\treturn decoded\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE1(\n",
      "  (encoder): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=64, out_features=18, bias=True)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): Linear(in_features=18, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (5): ReLU()\n",
      "    (6): Linear(in_features=256, out_features=784, bias=True)\n",
      "    (7): Sigmoid()\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "AE1_model=AE1()\n",
    "loss_func=nn.MSELoss()\n",
    "optimizer=torch.torch.optim.Adam(AE1_model.parameters(),lr=0.0001)\n",
    "print(AE1_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 0.1, Average SSIM: 0.0003\n",
      "Sigma: 0.3, Average SSIM: 0.0003\n",
      "Sigma: 0.5, Average SSIM: 0.0002\n",
      "\n",
      "\n",
      "Bottleneck Dimension: 12, Average SSIM: 0.0003\n",
      "Bottleneck Dimension: 20, Average SSIM: 0.0003\n",
      "Bottleneck Dimension: 28, Average SSIM: 0.0003\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train and evaluate model with varying sigma values\n",
    "sigmas = [0.1, 0.3, 0.5]\n",
    "# bottleneck_dim = 16\n",
    "for sigma in sigmas:\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        # AE1_model.train()\n",
    "        for images, _ in train_loader:\n",
    "            noisy_images = add_noise(images, sigma)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images = AE1_model(noisy_images)\n",
    "            loss = loss_func(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    average_ssim = calculate_average_ssim(AE1_model, test_loader, sigma)\n",
    "    print(f'Sigma: {sigma}, Average SSIM: {average_ssim:.4f}')\n",
    "\n",
    "print(\"\\n\")\n",
    "\n",
    "# Change bottleneck dimensionality and evaluate SSIM\n",
    "bottleneck_dims = [12, 20, 28]\n",
    "sigma = 0.1\n",
    "for bottleneck_dim in bottleneck_dims:\n",
    "    model = AE1(bottleneck_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, _ in train_loader:\n",
    "            noisy_images = add_noise(images, sigma)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images = model(noisy_images)\n",
    "            loss = criterion(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    average_ssim = calculate_average_ssim(model, test_loader, sigma)\n",
    "    print(f'Bottleneck Dimension: {bottleneck_dim}, Average SSIM: {average_ssim:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##2nd encoder decoder architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class AE2(nn.Module):\n",
    "    def __init__(self, latent_dim):\n",
    "        super(AE2, self).__init__()\n",
    "\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "            nn.Conv2d(16, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "\n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, kernel_size=2, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.ConvTranspose2d(16, 1, kernel_size=2, stride=2),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # Fully connected layer to map bottleneck to desired dimension\n",
    "        self.fc = nn.Linear(32 * 7 * 7, self.latent_dim)\n",
    "\n",
    "        # Fully connected layer to map bottleneck back to original dimension\n",
    "        self.fc_reverse = nn.Linear(self.latent_dim, 32 * 7 * 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x = self.encoder(x)\n",
    "        x = x.view(-1, 32 * 7 * 7)  # Flatten for fully connected layer\n",
    "        x = self.fc(x)  # Map to bottleneck dimension\n",
    "\n",
    "        # Decoder\n",
    "        x = self.fc_reverse(x)\n",
    "        x = x.view(-1, 32, 7, 7)  # Reshape for convolutional layers\n",
    "        x = self.decoder(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AE2(\n",
      "  (encoder): Sequential(\n",
      "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU()\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (4): ReLU()\n",
      "    (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (decoder): Sequential(\n",
      "    (0): ConvTranspose2d(32, 16, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (1): ReLU()\n",
      "    (2): ConvTranspose2d(16, 1, kernel_size=(2, 2), stride=(2, 2))\n",
      "    (3): Sigmoid()\n",
      "  )\n",
      "  (fc): Linear(in_features=1568, out_features=100, bias=True)\n",
      "  (fc_reverse): Linear(in_features=100, out_features=1568, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "AE2_model=AE2(100)\n",
    "loss_func=nn.MSELoss()\n",
    "optimizer=torch.torch.optim.Adam(AE2_model.parameters(),lr=0.0001)\n",
    "print(AE2_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigma: 0.1, Average SSIM: 0.8706\n",
      "Sigma: 0.3, Average SSIM: 0.8448\n",
      "Sigma: 0.5, Average SSIM: 0.8103\n",
      "Bottleneck Dimension: 8, Average SSIM: 0.7392\n",
      "Bottleneck Dimension: 16, Average SSIM: 0.8516\n",
      "Bottleneck Dimension: 32, Average SSIM: 0.9070\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Train and evaluate model with varying sigma values\n",
    "sigmas = [0.1, 0.3, 0.5]\n",
    "bottleneck_dim = 16\n",
    "for sigma in sigmas:\n",
    "    model =AE2(bottleneck_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, _ in train_loader:\n",
    "            noisy_images = add_noise(images, sigma)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images = model(noisy_images)\n",
    "            loss = criterion(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    average_ssim = calculate_average_ssim(model, test_loader, sigma)\n",
    "    print(f'Sigma: {sigma}, Average SSIM: {average_ssim:.4f}')\n",
    "\n",
    "# Change bottleneck dimensionality and evaluate SSIM\n",
    "bottleneck_dims = [8, 16, 32]\n",
    "sigma = 0.3\n",
    "for bottleneck_dim in bottleneck_dims:\n",
    "    model = AE2(bottleneck_dim)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    num_epochs = 5\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for images, _ in train_loader:\n",
    "            noisy_images = add_noise(images, sigma)\n",
    "            optimizer.zero_grad()\n",
    "            reconstructed_images = model(noisy_images)\n",
    "            loss = criterion(reconstructed_images, images)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "    average_ssim = calculate_average_ssim(model, test_loader, sigma)\n",
    "    print(f'Bottleneck Dimension: {bottleneck_dim}, Average SSIM: {average_ssim:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analysis:\n",
    "1. Impact of Gaussian Noise with Varying Sigma Values:\n",
    "Observation:\n",
    "As the sigma value increases (0.1 to 0.5), the average SSIM test score decreases.\n",
    "Higher sigma values indicate higher levels of noise, which can degrade image quality and impact reconstruction performance.\n",
    "Explanation:\n",
    "Higher sigma values lead to more intense noise, making it harder for the model to reconstruct the original image accurately.\n",
    "The decrease in SSIM score indicates reduced similarity between the original and reconstructed images as noise level increases.\n",
    "2. Impact of Bottleneck Dimensionality:\n",
    "Observation:\n",
    "Increasing the bottleneck dimensionality from 8 to 16 to 32 results in an increase in the average SSIM test score.\n",
    "Explanation:\n",
    "A higher bottleneck dimensionality allows the model to capture more information during encoding, facilitating better reconstruction during decoding.\n",
    "With a higher bottleneck dimensionality, the model has more capacity to preserve important features of the input image, resulting in improved reconstruction quality.\n",
    "The increase in SSIM score indicates higher similarity between the original and reconstructed images with a larger bottleneck dimensionality.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
