{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aEd7MnE5Cph",
        "outputId": "27451b48-91ab-46cf-e3d0-3802c887f9e0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZTAnfEW740Zi"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cjge05Zm40Zk"
      },
      "outputs": [],
      "source": [
        "transform =transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "trainset = torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/cifar-10-python', train=True, transform=transform)\n",
        "trainloader=torch.utils.data.DataLoader(trainset,batch_size=64,shuffle=True)\n",
        "\n",
        "testset=torchvision.datasets.CIFAR10(root='/content/drive/MyDrive/cifar-10-python',train=False,transform=transform)\n",
        "\n",
        "testloader=torch.utils.data.DataLoader(testset,batch_size=64,shuffle=True)\n",
        "\n",
        "\n",
        "# print((trainloader[0]))\n",
        "\n",
        "## traindata is containing 50000 images 3x32x32 and their label\n",
        "## test data is containing 10000 images of same dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DUvODe3R40Zl",
        "outputId": "ea2f3551-2d97-4634-97a5-a88bf238a5b9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started\n",
            "epoch= 1  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 36 % \n",
            "\n",
            "epoch= 2  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 39 % \n",
            "\n",
            "epoch= 3  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 41 % \n",
            "\n",
            "epoch= 4  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 44 % \n",
            "\n",
            "epoch= 5  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 45 % \n",
            "\n",
            "epoch= 6  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 45 % \n",
            "\n",
            "epoch= 7  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 45 % \n",
            "\n",
            "epoch= 8  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 47 % \n",
            "\n",
            "epoch= 9  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 46 % \n",
            "\n",
            "epoch= 10  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 47 % \n",
            "\n",
            "epoch= 11  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 48 % \n",
            "\n",
            "epoch= 12  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 49 % \n",
            "\n",
            "epoch= 13  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 49 % \n",
            "\n",
            "epoch= 14  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 49 % \n",
            "\n",
            "epoch= 15  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 49 % \n",
            "\n",
            "epoch= 16  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 49 % \n",
            "\n",
            "epoch= 17  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 50 % \n",
            "\n",
            "epoch= 18  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 50 % \n",
            "\n",
            "epoch= 19  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 51 % \n",
            "\n",
            "epoch= 20  is running\n",
            "training finished\n",
            "Accuracy of the network on the 10000 test images: 50 % \n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "## model defintion\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP,self).__init__()\n",
        "        self.flatten=nn.Flatten()\n",
        "        self.fc1=nn.Linear(32*32*3,256)\n",
        "        self.fc2=nn.Linear(256,128)\n",
        "        self.fc3=nn.Linear(128,10)\n",
        "\n",
        "    def forward(self,x):\n",
        "        x=self.flatten(x)\n",
        "        x=torch.relu(self.fc1(x))\n",
        "        x=torch.relu(self.fc2(x))\n",
        "        x=self.fc3(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "use_cuda=torch.cuda.is_available()\n",
        "device=torch.device(\"cuda:0\" if use_cuda else \"cpu\")\n",
        "if use_cuda:\n",
        "    model=MLP().cuda()\n",
        "\n",
        "loss_function=nn.CrossEntropyLoss()\n",
        "optimizer=optim.Adam(model.parameters(),lr=0.0001)\n",
        "\n",
        "\n",
        "Accuracies=[]\n",
        "\n",
        "## Training\n",
        "print(\"Training started\")\n",
        "for epoch in range(20):\n",
        "    print(\"epoch=\",epoch+1,\" is running\")\n",
        "    loss=0.0\n",
        "    for i,data in enumerate(trainloader,1):\n",
        "        inputs,labels=data[0].to(device),data[1].to(device)\n",
        "        # print(i)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ouputs=model(inputs)\n",
        "        loss=loss_function(ouputs,labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "        loss+=loss.item()\n",
        "    print(\"training finished\")\n",
        "\n",
        "# evaluation of the trained model\n",
        "    correct=0\n",
        "    total=0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images,labels=data[0].to(device),data[1].to(device)\n",
        "            outputs=model(images)\n",
        "            _,predicted=torch.max(outputs.data,1)\n",
        "            total+=labels.size(0)\n",
        "            correct+=(predicted==labels).sum().item()\n",
        "    print('Accuracy of the network on the 10000 test images: %d %% \\n' % (100 * correct / total))\n",
        "    Accuracies.append((100 * correct / total))\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vpj4qEW4oVWW",
        "outputId": "7d655b73-26a3-41d0-bcd4-9611cb37e37a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "MLP\n",
            "Accuracy of the MLP model on the test images: 51.47 %\n"
          ]
        }
      ],
      "source": [
        "print(\"MLP\")\n",
        "print('Accuracy of the MLP model on the test images: %.2f %%' % (100 * correct / total))\n",
        "# print(\"\\n\\n\")\n",
        "# print(correct,total)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-7gqglP40Zl",
        "outputId": "e5e89a48-e8c0-4e0b-9799-a2ab30ff9dff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "epoch= 1 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 52.74 %\n",
            "\n",
            "epoch= 2 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 58.94 %\n",
            "\n",
            "epoch= 3 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 63.87 %\n",
            "\n",
            "epoch= 4 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 64.95 %\n",
            "\n",
            "epoch= 5 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.36 %\n",
            "\n",
            "epoch= 6 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.82 %\n",
            "\n",
            "epoch= 7 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.12 %\n",
            "\n",
            "epoch= 8 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 69.83 %\n",
            "\n",
            "epoch= 9 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 69.04 %\n",
            "\n",
            "epoch= 10 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 69.53 %\n",
            "\n",
            "epoch= 11 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.76 %\n",
            "\n",
            "epoch= 12 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.91 %\n",
            "\n",
            "epoch= 13 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.03 %\n",
            "\n",
            "epoch= 14 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.31 %\n",
            "\n",
            "epoch= 15 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.38 %\n",
            "\n",
            "epoch= 16 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.63 %\n",
            "\n",
            "epoch= 17 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 67.71 %\n",
            "\n",
            "epoch= 18 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 68.11 %\n",
            "\n",
            "epoch= 19 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 67.85 %\n",
            "\n",
            "epoch= 20 is running\n",
            "Training finished\n",
            "Accuracy of the CNN model on the test images: 67.17 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define CNN architecture\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)\n",
        "        self.fc1 = nn.Linear(32 * 8 * 8, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 8 * 8)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Initialize the CNN model\n",
        "cnn_model = SimpleCNN()\n",
        "cnn_model.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the CNN model\n",
        "print(\"Training started...\")\n",
        "for epoch in range(20):  # Number of epochs\n",
        "    print(\"epoch=\",epoch+1,\"is running\")\n",
        "    running_loss = 0.0\n",
        "    for i, data in enumerate(trainloader, 0):\n",
        "        inputs, labels = data[0].to(device),data[1].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = cnn_model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(\"Training finished\")\n",
        "\n",
        "    # Evaluate the CNN model\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for data in testloader:\n",
        "            images, labels = data[0].to(device),data[1].to(device)\n",
        "            outputs = cnn_model(images)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the CNN model on the test images: %.2f %%\\n' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tkRGkfioPKU",
        "outputId": "bf7c77ba-2b22-4100-fe75-e5ee4d616e66"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "CNN Architecture\n",
            "Accuracy of the CNN model on the test images: 67.17 %\n",
            "\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"CNN Architecture\")\n",
        "print('Accuracy of the CNN model on the test images: %.2f %%' % (100 * correct / total))\n",
        "print(\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jdMPs-t5gNgS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vUy6JMw8ilpv"
      },
      "outputs": [],
      "source": [
        "\n",
        "# import torchvision.models as models\n",
        "\n",
        "# # Load pre-trained VGG model\n",
        "# vg_model = models.vgg16(pretrained=True)\n",
        "\n",
        "# print(vg_model)\n",
        "\n",
        "# # Check the output size of the last layer\n",
        "# num_features = vg_model.classifier[-1].in_features\n",
        "# print(\"Number of input features:\", num_features)\n",
        "\n",
        "# last_linear_idx = None\n",
        "# for idx, layer in enumerate(vg_model.classifier):\n",
        "#     if isinstance(layer, torch.nn.Linear):\n",
        "#         last_linear_idx = idx\n",
        "# print(\"last leayer index to be changed\",last_linear_idx)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H4XTqGTakxMc"
      },
      "outputs": [],
      "source": [
        "# import torch\n",
        "# import torchvision\n",
        "# import torchvision.transforms as transforms\n",
        "\n",
        "# # Check if GPU is available\n",
        "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# # Load pre-trained VGG model\n",
        "# vg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "\n",
        "# # Freeze the pre-trained layers\n",
        "# for param in vg_model.parameters():\n",
        "#     param.requires_grad = False\n",
        "\n",
        "\n",
        "# # Modify the last fully connected layer for 10 classes\n",
        "# vg_model.classifier[6] = nn.Linear(4096, 10)  # Assuming VGG11's classifier has 4096 input features\n",
        "\n",
        "\n",
        "# # Define transformations\n",
        "# transform = transforms.Compose([\n",
        "#     transforms.Resize(256),\n",
        "#     transforms.CenterCrop(224),  # Resize images to match VGG input size\n",
        "#     transforms.ToTensor(),\n",
        "#     transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "# ])\n",
        "\n",
        "# # Load CIFAR-10 dataset\n",
        "# testset = torchvision.datasets.CIFAR10(root=\"/content/drive/MyDrive/cifar-10-python\", train=False, transform=transform)\n",
        "# testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=True)\n",
        "\n",
        "# correct = 0\n",
        "# total = 0\n",
        "# with torch.no_grad():\n",
        "#     for data in testloader:\n",
        "#         images, labels = data[0].to(device), data[1].to(device)\n",
        "#         outputs = vg_model(images)\n",
        "#         _, predicted = torch.max(outputs, 1)\n",
        "#         total += labels.size(0)\n",
        "#         correct += (predicted == labels).sum().item()\n",
        "#         print(predicted,labels)\n",
        "\n",
        "# print('Accuracy of the VGG model on the test images: %.2f %%' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N1wYNCexvf81",
        "outputId": "0bf63ed1-399c-46ca-bc2e-e6b5835e674e"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=1000, bias=True)\n",
            "    (7): Linear(in_features=1000, out_features=100, bias=True)\n",
            "    (8): Linear(in_features=100, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained VGG model\n",
        "vg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for param in vg_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "features=list(vg_model.classifier.children())\n",
        "features.extend([nn.Linear(1000,100)])\n",
        "features.extend([nn.Linear(100,10)])\n",
        "vg_model.classifier = nn.Sequential(*features)\n",
        "print((vg_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "lptqbCZfksw2",
        "outputId": "fbb7a84a-ba9a-4d59-9617-2ef7d0437766"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "Training finished\n",
            "Accuracy of the VGG_16 model on the test images: 38.42 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define loss function and optimizer\n",
        "vg_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vg_model.classifier[7:].parameters(), lr=0.0001)\n",
        "# optimizer = optim.Adam(vg_model.classifier[7:].parameters(), lr=0.001)\n",
        "#\n",
        "# Train the CNN model\n",
        "print(\"Training started...\")\n",
        "\n",
        "vg_model.train()\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data[0].to(device),data[1].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = vg_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n",
        "# Evaluate the CNN model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device),data[1].to(device)\n",
        "        outputs = vg_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the VGG_16 model on the test images: %.2f %%\\n' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sX2xtuo5qKr5",
        "outputId": "05005251-dadf-4bc7-928c-14e685334d17"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/pytorch_vision_v0.10.0\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=VGG16_Weights.IMAGENET1K_V1`. You can also use `weights=VGG16_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "VGG(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (3): ReLU(inplace=True)\n",
            "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (6): ReLU(inplace=True)\n",
            "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (8): ReLU(inplace=True)\n",
            "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (11): ReLU(inplace=True)\n",
            "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (13): ReLU(inplace=True)\n",
            "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (15): ReLU(inplace=True)\n",
            "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (18): ReLU(inplace=True)\n",
            "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (20): ReLU(inplace=True)\n",
            "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (22): ReLU(inplace=True)\n",
            "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (25): ReLU(inplace=True)\n",
            "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (27): ReLU(inplace=True)\n",
            "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "    (29): ReLU(inplace=True)\n",
            "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=(7, 7))\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
            "    (1): ReLU(inplace=True)\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=4096, out_features=4096, bias=True)\n",
            "    (4): ReLU(inplace=True)\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=4096, out_features=10, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load pre-trained VGG model\n",
        "vg_model = torch.hub.load('pytorch/vision:v0.10.0', 'vgg16', pretrained=True)\n",
        "\n",
        "# Freeze the pre-trained layers\n",
        "for param in vg_model.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "features=list(vg_model.classifier.children())[:-1]\n",
        "features.extend([nn.Linear(4096,10)])\n",
        "vg_model.classifier = nn.Sequential(*features)\n",
        "print((vg_model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dKZfaUvlwm4B",
        "outputId": "3b393fe8-e4fb-47bf-8c65-a21032d6e77f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training started...\n",
            "Training finished\n",
            "Accuracy of the VGG_16 model on the test images: 33.41 %\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Define loss function and optimizer\n",
        "vg_model.to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(vg_model.classifier[-1].parameters(), lr=0.0001)\n",
        "# optimizer = optim.Adam(vg_model.classifier[7:].parameters(), lr=0.001)\n",
        "#\n",
        "# Train the CNN model\n",
        "print(\"Training started...\")\n",
        "\n",
        "vg_model.train()\n",
        "running_loss = 0.0\n",
        "for i, data in enumerate(trainloader, 0):\n",
        "    inputs, labels = data[0].to(device),data[1].to(device)\n",
        "    optimizer.zero_grad()\n",
        "    outputs = vg_model(inputs)\n",
        "    loss = criterion(outputs, labels)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    running_loss += loss.item()\n",
        "\n",
        "print(\"Training finished\")\n",
        "\n",
        "# Evaluate the CNN model\n",
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for data in testloader:\n",
        "        images, labels = data[0].to(device),data[1].to(device)\n",
        "        outputs = vg_model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print('Accuracy of the VGG_16 model on the test images: %.2f %%\\n' % (100 * correct / total))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PNXq0RyexWJX"
      },
      "source": [
        "Comparison of Test Set Accuracy and Loss Across Models\n",
        "Test Set Accuracy:\n",
        "\n",
        "MLP (Multi-Layer Perceptron): 51.47%\n",
        "CNN (Convolutional Neural Network): 67.17%\n",
        "VGG-based model (Pretrained VGG16): 33%\n",
        "Reasons Behind Differences in Performance:\n",
        "\n",
        "MLP:\n",
        "MLPs treat each pixel as an independent feature and do not capture spatial relationships in images.\n",
        "Consequently, MLPs may struggle to extract meaningful features from images, resulting in lower accuracy.\n",
        "\n",
        "CNN:\n",
        "CNNs leverage convolutional layers to capture local patterns and spatial hierarchies in images.\n",
        "This spatial understanding allows CNNs to learn more robust and discriminative features, leading to higher accuracy.\n",
        "VGG-based model:\n",
        "\n",
        "Fine-tuning a pre-trained VGG16 model .\n",
        "It requires careful adjustment of hyperparameters.\n",
        "I added linera layer to fine tune. It did not adapt well and give limited performance compare to MLP.\n",
        "But it took very less time to train. This is very useful to use in real time applicaion.\n",
        "\n",
        "\n",
        "Analysis:\n",
        "\n",
        "CNNs utilize convolutional layers to capture spatial structure, making them more effective for image-related tasks.\n",
        "MLPs, lacking this capability, struggle to extract meaningful features from images.\n",
        "Transfer Learning with VGG:\n",
        "\n",
        "In summary, CNNs outperform MLPs for image classification due to their ability to capture spatial structure. While transfer learning with VGG can be beneficial, it requires careful fine-tuning to achieve optimal performance on the target dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
